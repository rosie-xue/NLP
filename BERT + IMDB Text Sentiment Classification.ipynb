{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rosie-xue/NLP/blob/main/BERT%20%2B%20IMDB%20Text%20Sentiment%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtjABgegXPzs"
      },
      "source": [
        "# 简介\n",
        "![BERT](./bert.png)\n",
        "## 背景\n",
        "- BERT（Bidirectional Encoder Representations from Transformers）是由Google AI团队开发的预训练语言表示模型。\n",
        "\n",
        "## 核心概念\n",
        "- BERT是基于Transformer架构的，特别是它的编码器部分。\n",
        "- 它的创新之处在于使用了掩码语言模型（MLM）和下一个句子预测（NSP）两种训练任务。\n",
        "- BERT是双向的，这意味着它在处理每个词时会考虑整个句子的上下文。\n",
        "\n",
        "## 预训练和微调\n",
        "- BERT模型首先在大量文本数据上进行预训练，以学习语言的通用特征。\n",
        "- 然后，它可以通过微调（即在特定任务的数据集上进行额外训练）来适应各种NLP任务，如情感分析、问答、命名实体识别等。\n",
        "\n",
        "# BERT的训练任务\n",
        "\n",
        "## 掩码语言模型（MLM）\n",
        "- 在MLM任务中，BERT随机遮蔽输入序列中的一些词元（例如，用[MASK]标记替换），然后尝试预测它们。\n",
        "- 这种方式使BERT能够学习到词元的双向表示。\n",
        "\n",
        "## 下一个句子预测（NSP）\n",
        "- 在NSP任务中，模型获取两个句子作为输入，并预测第二个句子是否是第一个句子的下一个句子。\n",
        "- 这有助于模型理解句子间的关系，对于理解段落和文章非常重要。\n",
        "\n",
        "# BERT的影响和应用\n",
        "\n",
        "- 自发布以来，BERT已成为多种NLP任务的基准测试模型。\n",
        "- 它在多个NLP评测任务中取得了当时的最佳性能。\n",
        "- BERT已被广泛应用于各种语言处理任务，如文本分类、命名实体识别、问答系统等。\n",
        "- 它的出现也催生了一系列基于BERT架构的后续模型，如RoBERTa、ALBERT等。\n",
        "\n",
        "# 运行环境\n",
        "\n",
        "```\n",
        "torch==2.1.0+cu118\n",
        "transformers==4.35.2\n",
        "random\n",
        "numpy\n",
        "tqdm\n",
        "matplotlib\n",
        "scipy\n",
        "datasets\n",
        "sklearn\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc4Qg6qIXYP6",
        "outputId": "884a0ac8-1d4d-4d60-8a10-40c8ecc677bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "# 如使用colab平台，运行代码前请先安装datasets库\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGUhfmHYXPzv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from scipy.spatial.distance import cosine\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertForNextSentencePrediction, BertForSequenceClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CWfrY5Exjga"
      },
      "source": [
        "## 加载预训练的BERT模型和分词器\n",
        "\n",
        "- **分词器加载**：使用`BertTokenizer.from_pretrained('bert-base-uncased')`来加载一个预训练的BERT分词器。`bert-base-uncased`模型处理的文本是小写的。\n",
        "- **模型加载**：通过`BertModel.from_pretrained('bert-base-uncased', output_attentions=True)`加载与分词器对应的BERT模型。设置`output_attentions=True`可以在模型的输出中获得注意力权重。\n",
        "\n",
        "## 对文本进行分词\n",
        "\n",
        "- 使用分词器对文本进行分词，命令为：`tokenizer(text, return_tensors='pt')`。\n",
        "- `return_tensors='pt'`指定返回的数据格式是PyTorch张量。\n",
        "- 分词结果`encoded_input`是一个包含分词信息的字典，如token IDs（`input_ids`）和注意力掩码（`attention_mask`）。\n",
        "\n",
        "## 获取BERT模型的输出\n",
        "\n",
        "- 将分词结果作为输入传递给BERT模型，使用命令：`model(**encoded_input)`。\n",
        "- 模型的输出`output`包含了多个部分，例如最后一个隐藏层的状态（`last_hidden_state`）和注意力权重（如果`output_attentions`为True）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BL2jPRDEXPzw"
      },
      "outputs": [],
      "source": [
        "# 加载预训练的BERT模型和分词器\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
        "\n",
        "# 示例文本\n",
        "text = \"Hello, world. This is a BERT model example.\"\n",
        "\n",
        "# 对文本进行分词\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "# 获取BERT模型的输出\n",
        "output = model(**encoded_input)\n",
        "print(encoded_input['input_ids'])\n",
        "print(output.last_hidden_state[0, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PylXqWj8zzK1"
      },
      "source": [
        "# BERT 注意力可视化分析\n",
        "\n",
        "本文档提供了使用BERT模型进行注意力可视化的步骤说明。\n",
        "\n",
        "## 文本的分词处理\n",
        "\n",
        "- **分词**：使用 `tokenizer(text, return_tensors='pt')` 对文本 `text` 进行分词处理。`return_tensors='pt'` 表示返回的结果是PyTorch张量格式。\n",
        "\n",
        "## 获取模型输出和注意力\n",
        "\n",
        "- **模型输出**：通过 `model(**inputs)` 获取BERT模型对输入文本的输出。这里的输出包括最后一层的隐藏状态和注意力矩阵。\n",
        "- **注意力矩阵**：`outputs.attentions` 是模型输出的注意力部分，它是一个多维数组，包含了模型每一层的注意力头的信息。\n",
        "\n",
        "## 选择可视化的层和头\n",
        "\n",
        "- **指定层和头**：通过设置 `layer = 0` 和 `head = 0`，我们选择模型的第一层和第一个注意力头进行可视化。\n",
        "\n",
        "## 获取并可视化注意力矩阵\n",
        "\n",
        "- **可视化注意力矩阵**：使用 `matplotlib` 的 `imshow` 函数将注意力矩阵可视化。这个矩阵表示了输入中的每个词对其他词的注意力分布。\n",
        "\n",
        "## 设置图表并显示\n",
        "\n",
        "- **获取分词文本**：使用 `tokenizer.convert_ids_to_tokens()` 将token IDs转换回词汇。\n",
        "- **设置图表标签**：设置x轴和y轴的标签为分词后的文本，使用 `plt.xticks` 和 `plt.yticks` 添加标签。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "mC8a7UTsXPzx",
        "outputId": "b6b91009-40c0-4671-8633-7add85d12c8f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 要分析的文本\n",
        "text = \"Hello, world. This is a BERT model example.\"\n",
        "\n",
        "# 对文本进行分词处理\n",
        "inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "# 获取模型的输出，包括注意力\n",
        "outputs = model(**inputs)\n",
        "attentions = outputs.attentions\n",
        "\n",
        "# 选择要可视化的层和头\n",
        "layer = 0\n",
        "head = 0\n",
        "\n",
        "# 获取注意力矩阵（只用获取 输出 的 注意力 部分）\n",
        "attention = attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "# 可视化\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(attention)\n",
        "\n",
        "# 设置图表\n",
        "tokenized_text = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "plt.xticks(range(len(tokenized_text)), tokenized_text, rotation=90)\n",
        "plt.yticks(range(len(tokenized_text)), tokenized_text)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VFMXMNr11SW"
      },
      "source": [
        "# BERT MLM\n",
        "\n",
        "## 初始化模型和分词器\n",
        "\n",
        "- **分词器加载**：使用 `BertTokenizer.from_pretrained('bert-base-uncased')` 来加载一个预训练的BERT分词器。该模型处理的文本是小写的。\n",
        "- **MLM模型加载**：通过 `BertForMaskedLM.from_pretrained('bert-base-uncased')` 加载用于MLM任务的BERT模型。这个特定的模型经过训练，专门用于预测掩码位置上的词。\n",
        "- **设置评估模式**：使用 `mlm_model.eval()` 将模型设置为评估模式，这意味着在预测时不会更新模型的权重。\n",
        "\n",
        "## 文本分词处理\n",
        "\n",
        "- **定义文本**：定义一个字符串变量 `text`，内容为：\"The capital of France is [MASK].\"。这段文本包含一个掩码标记 [MASK]，BERT模型将被用来预测这个位置的词。\n",
        "\n",
        "- **分词处理**：使用 `tokenizer(text, return_tensors=\"pt\")` 对文本进行分词处理，并将其转换为模型所需的格式（PyTorch张量）。\n",
        "\n",
        "## 模型预测\n",
        "\n",
        "- **预测掩码词**：在不计算梯度的情况下（`torch.no_grad()`），通过 `mlm_model(**inputs)` 对掩码位置进行预测。`outputs.logits` 是模型的原始输出。\n",
        "\n",
        "## 获取并解析预测结果\n",
        "\n",
        "- **定位掩码索引**：使用 `torch.where()` 找到输入中 [MASK] 位置的索引。\n",
        "- **提取预测的token ID**：使用 `argmax()` 从模型输出中提取概率最高的token ID。\n",
        "- **解码token ID**：通过 `tokenizer.decode()` 将预测的token ID解码为词汇。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vdWP20bXPzx",
        "outputId": "cac542e7-f3e1-4ed3-f9e4-41a06c796d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The capital of France is [MASK].\n",
            "Predicted Word: paris\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "mlm_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "mlm_model.eval()  # 将模型设置为评估模式\n",
        "text = \"The capital of France is [MASK].\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = mlm_model(**inputs)\n",
        "    predictions = outputs.logits\n",
        "masked_index = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "predicted_token_id = predictions[0, masked_index].argmax(axis=1)\n",
        "predicted_token = tokenizer.decode(predicted_token_id)\n",
        "\n",
        "print(f\"Original Text: {text}\")\n",
        "print(f\"Predicted Word: {predicted_token}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3_HSB1z3ZB5"
      },
      "source": [
        "# BERT NSP\n",
        "\n",
        "## 初始化模型和分词器\n",
        "\n",
        "- **分词器加载**：使用 `BertTokenizer.from_pretrained('bert-base-uncased')` 来加载一个预训练的BERT分词器。该模型处理的文本是小写的。\n",
        "- **NSP模型加载**：通过 `BertForNextSentencePrediction.from_pretrained('bert-base-uncased')` 加载用于NSP任务的BERT模型。这个特定的模型经过训练，专门用于预测一个句子是否是另一个句子的下一个句子。\n",
        "- **设置评估模式**：使用 `nsp_model.eval()` 将模型设置为评估模式，这意味着在预测时不会更新模型的权重。\n",
        "\n",
        "## 为句子创建编码和分段标记\n",
        "\n",
        "- **定义文本**：定义两个字符串变量 `text_a` 和 `text_b`，分别包含两个待比较的句子。示例中的句子分别为：\"The quick brown fox jumps over the lazy dog\" 和 \"The dog is named Rover\"。\n",
        "\n",
        "- **分词处理**：使用 `tokenizer(text_a, text_b, return_tensors=\"pt\")` 对两个句子进行分词处理，并将其转换为模型所需的格式（PyTorch张量）。\n",
        "\n",
        "## 模型预测\n",
        "\n",
        "- **预测关系**：在不计算梯度的情况下（`torch.no_grad()`），通过 `nsp_model(**encoding, return_dict=True)` 对两个句子间的关系进行预测。`outputs.logits` 是模型的原始输出。\n",
        "\n",
        "## 获取并解析预测结果\n",
        "\n",
        "- **计算概率**：使用 `torch.nn.functional.softmax()` 将模型的输出转换为概率。\n",
        "- **提取预测的标签**：使用 `torch.argmax()` 从概率中提取预测标签。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIU8jwROXPzx",
        "outputId": "0d54d8a6-8dc6-4da4-c010-33f9a04756fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text A: The quick brown fox jumps over the lazy dog\n",
            "Text B: The dog is named Rover\n",
            "Prediction: Not Next\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "nsp_model.eval()  # 将模型设置为评估模式\n",
        "text_a = \"The quick brown fox jumps over the lazy dog\"\n",
        "text_b = \"The dog is named Rover\"\n",
        "\n",
        "# 为句子创建编码和分段标记\n",
        "encoding = tokenizer(text_a, text_b, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = nsp_model(**encoding, return_dict=True)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "# 获取预测结果\n",
        "predicted_label = torch.argmax(probabilities, dim=1)\n",
        "labels = ['Not Next', 'Next']\n",
        "print(f\"Text A: {text_a}\")\n",
        "print(f\"Text B: {text_b}\")\n",
        "print(f\"Prediction: {labels[predicted_label]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REceqnPA_97H"
      },
      "source": [
        "# 基于BERT的IMDB文本情感分类\n",
        "\n",
        "## 初始化模型和分词器\n",
        "\n",
        "- **分词器加载**：使用 `BertTokenizer.from_pretrained(\"bert-base-uncased\")` 加载一个分词器，用于将文本转换为模型能理解的格式。\n",
        "- **分类模型加载**：通过 `BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")` 加载用于序列分类（情感分类）的BERT模型。\n",
        "\n",
        "## 加载和预处理数据集\n",
        "\n",
        "- **加载IMDB数据集**：使用 `load_dataset(\"imdb\")` 加载IMDB电影评论数据集，它包含了电影评论的文本和相应的情感标签（正面或负面）。\n",
        "- **随机选择样本**：从训练集和测试集中各随机选择1000条样本进行训练和评估。\n",
        "- **创建DataLoader**：使用 `DataLoader` 创建训练集和测试集的迭代器，设置批量大小和是否打乱数据。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbD5sfsVCGEo"
      },
      "source": [
        "`BertForSequenceClassification` 是基于 BERT (Bidirectional Encoder Representations from Transformers) 模型的一个变种，专门用于序列分类任务，例如情感分析或意图识别。它是由 Hugging Face's Transformers 库提供的。\n",
        "\n",
        "## 初始化参数\n",
        "\n",
        "初始化 `BertForSequenceClassification` 时的关键参数包括：\n",
        "\n",
        "- **config**: `BertConfig` 对象，包含模型配置信息。\n",
        "- **num_labels**: 分类任务的类别数量。例如，对于二元情感分析，该值为 2。\n",
        "\n",
        "## 模型的输入\n",
        "\n",
        "模型的主要输入参数包括：\n",
        "\n",
        "- **input_ids**: 形状为 `(batch_size, sequence_length)` 的张量，包含编码后的输入序列。\n",
        "- **attention_mask**: 形状相同的张量，用于区分真实数据和填充。\n",
        "- **token_type_ids**: 当处理两个序列时，此张量用于区分它们。\n",
        "\n",
        "## 模型的输出\n",
        "\n",
        "模型输出是一个元组，主要包含：\n",
        "\n",
        "- **logits**: 形状为 `(batch_size, num_labels)` 的张量，表示每个类别的预测得分。\n",
        "- **loss** (如果提供了标签): 用于训练的损失值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYNt1r6-55fc"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "cls_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "random_train_indices = random.sample(range(len(dataset[\"train\"])), 1000)\n",
        "random_test_indices = random.sample(range(len(dataset[\"test\"])), 1000)\n",
        "train_dataset = dataset[\"train\"].select(random_train_indices)\n",
        "test_dataset = dataset[\"test\"].select(random_test_indices)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwTqmeWzA0Vo"
      },
      "source": [
        "# 模型训练与评估\n",
        "\n",
        "## 训练模型函数 `train_model`\n",
        "\n",
        "此函数用于训练模型。\n",
        "\n",
        "### 参数\n",
        "\n",
        "- `model`: 要训练的BERT模型。\n",
        "- `train_loader`: 训练数据的DataLoader。\n",
        "- `optimizer`: 优化器，用于更新模型的权重。\n",
        "- `lr_scheduler`: 学习率调度器，用于调整学习率。\n",
        "- `num_epochs`: 训练的总轮次。\n",
        "- `device`: 训练使用的设备，如CPU或GPU。\n",
        "- `criterion`: 损失函数。\n",
        "\n",
        "### 功能\n",
        "\n",
        "1. **设置模型为训练模式**：确保模型在训练过程中更新权重。\n",
        "2. **遍历每个epoch**：循环处理每个训练轮次。\n",
        "3. **数据处理和训练**：对每个batch的数据进行处理，包括文本的分词和转换为张量，然后在指定设备上进行训练。\n",
        "4. **反向传播和优化**：计算损失，并进行反向传播来优化模型参数。\n",
        "\n",
        "## 评估模型函数 `evaluate_model`\n",
        "\n",
        "此函数用于评估模型的性能。\n",
        "\n",
        "### 参数\n",
        "\n",
        "- `model`: 要评估的BERT模型。\n",
        "- `test_loader`: 测试数据的DataLoader。\n",
        "- `device`: 评估使用的设备，如CPU或GPU。\n",
        "\n",
        "### 功能\n",
        "\n",
        "1. **设置模型为评估模式**：确保模型在评估过程中不更新权重。\n",
        "2. **遍历测试数据**：对测试数据集的每个batch进行迭代。\n",
        "3. **数据处理和预测**：与训练过程类似，对每个batch的数据进行处理，然后进行预测。\n",
        "4. **计算准确率**：收集所有预测结果，并与真实标签比较，计算模型的准确率。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6G3f3lUYU7W"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, optimizer, lr_scheduler, num_epochs, device, criterion):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in tqdm(train_loader):\n",
        "            # TODO: 使用tokenizer处理输入数据，并加载至device\n",
        "            inputs = tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "            input_ids = inputs[\"input_ids\"].to(device)\n",
        "            attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "            # TODO: 将标签数据加载至device..\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            # TODO: 计算模型输出和损失\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            # 反向传播和优化\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    references = []\n",
        "    for batch in tqdm(test_loader):\n",
        "        # TODO: 使用tokenizer处理输入数据，并加载至device\n",
        "        inputs = tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # TODO: 将标签数据加载至device..\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        # 计算模型输出\n",
        "        with torch.no_grad():\n",
        "            # TODO: 计算模型输出\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # TODO: 提取logits并生成预测结果\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits,dim=1)\n",
        "        predictions.extend(preds.tolist())\n",
        "\n",
        "        # 收集真实标签用于后续计算准确率\n",
        "        references.extend(batch[\"label\"].tolist())\n",
        "\n",
        "    # 计算准确率\n",
        "    return accuracy_score(references, predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20ST_CTNYFiK",
        "outputId": "d1f22559-e78e-45e4-9bd6-e0bc14fcd874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:33<00:00,  3.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy before fine-tuning: 0.508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 确保模型在CPU或GPU上运行\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cls_model.to(device)\n",
        "\n",
        "# 微调前评估\n",
        "pre_tuning_accuracy = evaluate_model(cls_model, test_loader, device)\n",
        "print(f\"Accuracy before fine-tuning: {pre_tuning_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9RA4bXlX74h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac112a94-6039-4b1d-ec4a-df07b4415b1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100%|██████████| 125/125 [01:29<00:00,  1.40it/s]\n"
          ]
        }
      ],
      "source": [
        "# 设置训练参数\n",
        "optimizer = AdamW(cls_model.parameters(), lr=2e-5)\n",
        "num_epochs = 1\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 训练模型\n",
        "train_model(cls_model, train_loader, optimizer, lr_scheduler, num_epochs, device, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 微调后评估\n",
        "post_tuning_accuracy = evaluate_model(cls_model, test_loader, device)\n",
        "print(f\"Accuracy after fine-tuning: {post_tuning_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFTc49NZ-i3t",
        "outputId": "4061b3a2-876d-4c51-dd21-63cd84de868a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:30<00:00,  4.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy after fine-tuning: 0.889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFvI1Z0n_dce"
      },
      "source": [
        "# QUESTIONS：\n",
        "1. **注意力机制的优越性是什么？能提供一个实际应用场景吗？**\n",
        "- 注意力机制通过允许模型在每一步都能够直接访问序列的任意部分，有效地缓解了处理长序列数据时，可能会出现梯度消失或梯度爆炸的问题。\n",
        "- 注意力机制可以为模型的每个输出提供一个权重，这些权重反映了输入序列中各部分对该输出的重要性。这种特性提高了模型的解释性，使我们能够更清楚地理解模型的决策过程。\n",
        "- 自注意力机制（Self-Attention）可以并行化计算，而不像RNN那样需要逐步处理序列数据，这显著提高了训练和推理的效率。\n",
        "- 注意力机制能够捕捉序列中元素之间的全局关系，不再局限于局部信息。这对于理解和生成上下文丰富的序列非常重要。\n",
        "- **应用场景：**机器翻译。在传统的Seq2Seq模型中，翻译长句子可能会遇到困难，因为句子的前半部分信息在生成译文时容易丢失。引入注意力机制后，模型可以在每个翻译步骤中“注意”到源句子中的相关部分，从而生成更准确和流畅的译文。\n",
        "   \n",
        "2. **简述MLM和NSP的训练过程，并分析二者在预训练过程中的作用。**\n",
        "- **(1) MLM的训练过程:**\n",
        "- 掩码生成：在输入序列中，随机选择并遮盖一部分词汇（通常是15%的词汇），用特殊的掩码符号\"[MASK]\"替代。同时，一部分被遮盖的词将被替换为随机的其他词（通常是80%的替换概率），另一部分维持原样。\n",
        "- 输入构建：将生成的带有掩码的句子作为输入传入预训练模型中。\n",
        "预测与损失计算：模型会对句子中的每个词进行处理，并尝试预测被遮盖的词。对于每个被遮盖的词，与原始句子中相应的位置进行比较，计算预测词与原始词之间的交叉熵损失。注意，只有被遮盖的词的预测结果会被用来计算损失，其他位置的预测结果会被忽略。\n",
        "- 反向传播和优化：根据计算得到的损失，使用反向传播算法更新模型中的参数。\n",
        "- **作用**：通过随机遮盖部分词汇并训练模型预测这些词汇，MLM有助于模型学习到文本的语义信息。这种方式使模型不仅能够理解单词的含义，还能考虑上下文的语义信息，从而提高模型的语义表达能力。\n",
        "- **(2) NSP的训练过程**\n",
        "- 数据构建：构建训练数据对，将两个句子按照特定格式进行组合，如\"[CLS] 句子 A [SEP] 句子 B [SEP]\"。[CLS] token表示句子级别的语义信息，[SEP] token用于分隔不同句子。\n",
        "- 连续性标记：对于每一对句子，通过判断它们在原文本中是否连续来设置标签。如果是连续的，标签为1；如果不连续，标签为0。\n",
        "- 输入处理：将组合后的句子转换为适合预训练模型训练的向量表示，然后输入模型。\n",
        "预测与损失计算：模型在进行预测时根据学习到的[CLS] token的表示来判断输入句子的连续性。通过比较预测结果与真实标签，计算损失。\n",
        "- 反向传播和优化：根据计算得到的损失，使用反向传播算法更新模型中的参数。\n",
        "- **作用**：通过预测两个句子是否连续出现，NSP有助于模型理解句子间的逻辑关系。这有助于模型在后续任务中更好地处理句子间的关系，如问答、文本摘要等。\n",
        "\n",
        "3. **解释微调（Fine-tuning）在BERT中的作用。为什么微调是BERT在各种NLP任务中成功的关键？**\n",
        "- BERT作为一个预训练的通用语言模型，已经在大规模无监督数据上学习了基本的语言表示能力。然而，对于具体的NLP任务，如文本分类、问答系统等，需要模型具备针对该任务的特定能力。微调正是通过在有监督的特定任务数据上对BERT的模型参数进行微小的调整，使得BERT能够更好地适应并优化在该任务上的表现。这种调整通常包括改变模型的输出层、微调某些层或修改激活函数等。\n",
        "\n",
        "- 预训练与微调的结合：BERT的预训练阶段使其具备了丰富的语言表示能力，而微调则能够将这种能力转化为在特定任务上的高性能表现。这种预训练与微调的结合方式使得BERT在各种NLP任务中都能够表现出色。\n",
        "- 迁移学习的效果：由于BERT在预训练阶段已经学习到了大量的语言知识，微调可以在训练数据较少的情况下起到迁移学习的作用。这意味着，即使对于某些数据较少的NLP任务，BERT也能够通过微调来充分利用预训练阶段学到的知识，进而提升模型的泛化能力和性能。\n",
        "- 定制化的优化：微调允许BERT根据具体任务的需求进行定制化的优化。通过调整模型的输出层、微调某些层或修改激活函数等，可以使BERT在特定任务上取得更好的性能。这种定制化的优化方式是BERT在各种NLP任务中成功的关键之一。\n",
        "\n",
        "4. **本次实践内容中基于BERT的IMDB文本情感分类的技术路线是否可以进一步优化？**\n",
        "- **数据预处理**：可以进一步优化文本预处理步骤，比如更精细地处理标点符号、停用词、文本标准化等，以提高输入数据的质量。\n",
        "- **模型改进**：可以尝试在BERT模型的基础上引入其他技术，如注意力机制、记忆网络等，以增强模型对文本特征的捕捉能力。可以考虑使用BERT的变体，如RoBERTa、DistilBERT等，这些变体在某些任务上可能表现更佳。\n",
        "- **训练策略**：可以调整训练策略，如采用不同的学习率调度策略、优化算法等，以加速模型收敛并提高性能。尝试使用迁移学习的方法，先在大量无标签数据上进行预训练，再在IMDB数据集上进行微调，以提升模型的泛化能力。\n",
        "- **集成学习**：可以考虑使用集成学习的方法，将多个基于BERT的模型或其他类型的模型进行集成，以获得更稳健和准确的预测结果。\n",
        "- **超参数调优**：对模型的超参数进行细致的调优，包括学习率、批次大小、训练轮次等，以找到最佳的性能配置。。\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}